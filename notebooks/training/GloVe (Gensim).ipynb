{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (Gensim)\n",
    "\n",
    "For looking at word vectors, we'll use **Gensim**. **Gensim** isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.   We gonna use **GloVe** embeddings, downloaded at [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Word2Vec(corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model in Word2Vec format for later use\n",
    "loaded_model.wv.save_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)\n",
    "\n",
    "# Load the model back for testing\n",
    "model = KeyedVectors.load_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return the vectors\n",
    "model['coffee'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beer', 0.971849262714386),\n",
       " ('mud', 0.9710080027580261),\n",
       " ('cloth', 0.9670559167861938),\n",
       " ('bid', 0.9668721556663513),\n",
       " ('elephants', 0.9642710089683533),\n",
       " ('putt', 0.9636533856391907),\n",
       " ('bone', 0.9619432091712952),\n",
       " ('target', 0.961246132850647),\n",
       " ('cab', 0.9603875875473022),\n",
       " ('command', 0.960010826587677)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comparison', 0.9648732542991638),\n",
       " ('congregation', 0.9622362852096558),\n",
       " ('character', 0.9620834589004517),\n",
       " ('danger', 0.9600833058357239),\n",
       " ('analysis', 0.9596493244171143),\n",
       " ('proof', 0.958351194858551),\n",
       " ('tradition', 0.9560423493385315),\n",
       " ('ultimate', 0.9554263949394226),\n",
       " ('humanity', 0.9541069865226746),\n",
       " ('theory', 0.9535269141197205)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('energy', 0.9677100777626038),\n",
       " ('frame', 0.957679271697998),\n",
       " ('annual', 0.9556729197502136),\n",
       " ('location', 0.9556114077568054),\n",
       " ('central', 0.9546933770179749),\n",
       " ('forming', 0.9532230496406555),\n",
       " ('enterprise', 0.9522820711135864),\n",
       " ('diffusion', 0.9509390592575073),\n",
       " ('aid', 0.9498111009597778),\n",
       " ('farm', 0.9496405720710754)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiple meanings....\n",
    "model.most_similar(\"plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers: 0.9490\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organized: 0.9657\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'code'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "We have talked about this in the last class.  Here we can conveniently use `distance` to find the cosine distance between two words. Note that distance = 1 - similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13251203298568726, 0.08375269174575806)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dog\"\n",
    "w2 = \"cat\"\n",
    "w3 = \"fruit\"\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#dog is much closer to cat then dog to fruit\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28125160932540894, 0.08064490556716919)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"happy\" # synonym 1\n",
    "w2 = \"cheerful\" # synonym 2\n",
    "w3 = \"sad\" # antonym\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#$w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\"!!\n",
    "#those similarlity does not handle antonym....\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "You guys....one very important thing is that NLP models are biased.....very bad...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tract', 0.9729620814323425),\n",
      " ('Control', 0.9661584496498108),\n",
      " ('African', 0.9654223918914795),\n",
      " ('Model', 0.9647645950317383),\n",
      " ('spectacular', 0.9641609787940979),\n",
      " ('megatons', 0.963944137096405),\n",
      " ('splendid', 0.9634275436401367),\n",
      " ('biggest', 0.9634273052215576),\n",
      " ('assembly', 0.9632208943367004),\n",
      " ('WTV', 0.9631377458572388)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(model.most_similar(positive=['woman', 'worker'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('originate', 0.878252387046814),\n",
      " ('interfere', 0.8769871592521667),\n",
      " ('Everyone', 0.8759522438049316),\n",
      " ('promise', 0.8743034601211548),\n",
      " ('stress', 0.8733978271484375),\n",
      " ('stain', 0.8725126385688782),\n",
      " ('concentrate', 0.8714606165885925),\n",
      " ('Hiroshima', 0.868313193321228),\n",
      " ('greatness', 0.8671536445617676),\n",
      " ('sign', 0.8631444573402405)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=['man', 'worker'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('device', 0.9400398135185242),\n",
      " ('Lord', 0.9387309551239014),\n",
      " ('abandoned', 0.9359316229820251),\n",
      " ('accepted', 0.935127317905426),\n",
      " ('ended', 0.9344491362571716),\n",
      " ('conductor', 0.9341519474983215),\n",
      " ('magnificent', 0.9319649934768677),\n",
      " ('won', 0.929638147354126),\n",
      " ('generation', 0.9288925528526306),\n",
      " ('released', 0.9281613230705261)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('economic', 0.9032350182533264),\n",
      " ('national', 0.8749502897262573),\n",
      " ('assistance', 0.869625985622406),\n",
      " ('social', 0.8682600855827332),\n",
      " ('management', 0.86331707239151),\n",
      " ('facilities', 0.8590260744094849),\n",
      " ('industrial', 0.8562296032905579),\n",
      " ('practical', 0.8549009561538696),\n",
      " ('discrimination', 0.8546333909034729),\n",
      " ('economies', 0.8498783707618713)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"human\", \"code\"], negative=[\"woman\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'specifically'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'development'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('bird', 'fly', 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coke\n"
     ]
    }
   ],
   "source": [
    "#which word in the list does not belong\n",
    "print(model.doesnt_match(\"coke pepsi sprite water\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickle of the model\n",
    "import pickle\n",
    "\n",
    "with open('glove_gensim.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
