{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (Gensim)\n",
    "\n",
    "For looking at word vectors, we'll use **Gensim**. **Gensim** isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.   We gonna use **GloVe** embeddings, downloaded at [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pprint\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Word2Vec(corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model in Word2Vec format for later use\n",
    "loaded_model.wv.save_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)\n",
    "\n",
    "# Load the model back for testing\n",
    "model = KeyedVectors.load_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return the vectors\n",
    "model['coffee'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beer', 0.9718764424324036),\n",
       " ('mud', 0.9709972143173218),\n",
       " ('cloth', 0.9670196771621704),\n",
       " ('bid', 0.9667823314666748),\n",
       " ('elephants', 0.9642915725708008),\n",
       " ('putt', 0.963595986366272),\n",
       " ('bone', 0.9619939923286438),\n",
       " ('target', 0.9612566828727722),\n",
       " ('cab', 0.9603590965270996),\n",
       " ('command', 0.960012674331665)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comparison', 0.9648641347885132),\n",
       " ('congregation', 0.962188720703125),\n",
       " ('character', 0.9620252251625061),\n",
       " ('danger', 0.9601103663444519),\n",
       " ('analysis', 0.9596117734909058),\n",
       " ('proof', 0.9583585858345032),\n",
       " ('tradition', 0.9559952020645142),\n",
       " ('ultimate', 0.9554356336593628),\n",
       " ('humanity', 0.9540989995002747),\n",
       " ('theory', 0.9535123705863953)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('energy', 0.9676952958106995),\n",
       " ('frame', 0.9576594829559326),\n",
       " ('annual', 0.9557315111160278),\n",
       " ('location', 0.9555957913398743),\n",
       " ('central', 0.9546601176261902),\n",
       " ('forming', 0.953223466873169),\n",
       " ('enterprise', 0.9522941708564758),\n",
       " ('diffusion', 0.9510025978088379),\n",
       " ('aid', 0.949894368648529),\n",
       " ('farm', 0.9496124386787415)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiple meanings....\n",
    "model.most_similar(\"plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers: 0.9490\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organized: 0.9657\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'code'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "We have talked about this in the last class.  Here we can conveniently use `distance` to find the cosine distance between two words. Note that distance = 1 - similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13264435529708862, 0.08378010988235474)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dog\"\n",
    "w2 = \"cat\"\n",
    "w3 = \"fruit\"\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#dog is much closer to cat then dog to fruit\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2810835838317871, 0.08059042692184448)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"happy\" # synonym 1\n",
    "w2 = \"cheerful\" # synonym 2\n",
    "w3 = \"sad\" # antonym\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#$w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\"!!\n",
    "#those similarlity does not handle antonym....\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "You guys....one very important thing is that NLP models are biased.....very bad...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tract', 0.9729741215705872),\n",
      " ('Control', 0.9661687016487122),\n",
      " ('African', 0.9653110504150391),\n",
      " ('Model', 0.9647584557533264),\n",
      " ('spectacular', 0.964256227016449),\n",
      " ('megatons', 0.9639296531677246),\n",
      " ('biggest', 0.9634395837783813),\n",
      " ('splendid', 0.9634092450141907),\n",
      " ('WTV', 0.9632601737976074),\n",
      " ('assembly', 0.9631929993629456)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=['woman', 'worker'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('originate', 0.8781259059906006),\n",
      " ('interfere', 0.8769797682762146),\n",
      " ('Everyone', 0.8759639859199524),\n",
      " ('promise', 0.8743672370910645),\n",
      " ('stress', 0.8733637928962708),\n",
      " ('stain', 0.872747004032135),\n",
      " ('concentrate', 0.8715300559997559),\n",
      " ('Hiroshima', 0.8685178160667419),\n",
      " ('greatness', 0.8672943115234375),\n",
      " ('sign', 0.8632235527038574)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=['man', 'worker'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('device', 0.9401828646659851),\n",
      " ('Lord', 0.9385852217674255),\n",
      " ('abandoned', 0.9358891248703003),\n",
      " ('accepted', 0.9350557327270508),\n",
      " ('ended', 0.9345287084579468),\n",
      " ('conductor', 0.9343954920768738),\n",
      " ('magnificent', 0.9321774840354919),\n",
      " ('won', 0.9295370578765869),\n",
      " ('generation', 0.9290923476219177),\n",
      " ('released', 0.9281633496284485)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('economic', 0.9033142924308777),\n",
      " ('national', 0.8750497102737427),\n",
      " ('assistance', 0.8694512844085693),\n",
      " ('social', 0.868399441242218),\n",
      " ('management', 0.8633675575256348),\n",
      " ('facilities', 0.8589795231819153),\n",
      " ('industrial', 0.8562678098678589),\n",
      " ('practical', 0.8548918962478638),\n",
      " ('discrimination', 0.8547709584236145),\n",
      " ('economies', 0.8499754667282104)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"human\", \"code\"], negative=[\"woman\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'specifically'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'development'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('bird', 'fly', 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coke\n"
     ]
    }
   ],
   "source": [
    "#which word in the list does not belong\n",
    "print(model.doesnt_match(\"coke pepsi sprite water\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickle of the model\n",
    "import pickle\n",
    "\n",
    "with open('../../app/models/glove_gensim/glove_gensim.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
