{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [[word for word in sent if word.lower() not in stop_words] for sent in corpus]\n",
    "\n",
    "# Remove punctuation from corpus\n",
    "corpus = [[word for word in sent if word not in punctuation] for sent in corpus]\n",
    "\n",
    "# Remove empty sentences\n",
    "corpus = [sent for sent in corpus if len(sent) > 0]\n",
    "\n",
    "# Remove sentences with less than 5 words\n",
    "corpus = [sent for sent in corpus if len(sent) >= 5]\n",
    "\n",
    "# Remove sentences with more than 20 words\n",
    "corpus = [sent for sent in corpus if len(sent) <= 20]\n",
    "\n",
    "# Remove rare words\n",
    "word_freq = Counter([word for sent in corpus for word in sent])\n",
    "corpus = [[word for word in sent if word_freq[word] > 5] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4565"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[len(index2word) - 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc) - window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = rest of the words\n",
    "            outside_start =  i - window_size\n",
    "            outside_end =  i + window_size + 1\n",
    "\n",
    "            for j in range(outside_start, outside_end):\n",
    "                if i != j:  # Skip the center word\n",
    "                    outside = word2index[doc[j]]\n",
    "                    skipgrams.append([center, outside])\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10442],\n",
       "       [ 8251]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape  #batch_size 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10583"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocabs), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.LongTensor(x)\n",
    "embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 10580, 10581, 10582],\n",
       "        [    0,     1,     2,  ..., 10580, 10581, 10582]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(10583, 2)\n",
       "  (embedding_outside): Embedding(10583, 2)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Skipgram(voc_size, 2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2634, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Loss: 11.805612 | Time: 1m 10s\n",
      "Epoch: 200 | Loss: 10.212271 | Time: 2m 20s\n",
      "Epoch: 300 | Loss: 10.971835 | Time: 3m 33s\n",
      "Epoch: 400 | Loss: 12.360687 | Time: 4m 57s\n",
      "Epoch: 500 | Loss: 11.242234 | Time: 6m 18s\n",
      "Epoch: 600 | Loss: 9.940428 | Time: 7m 37s\n",
      "Epoch: 700 | Loss: 9.953606 | Time: 8m 54s\n",
      "Epoch: 800 | Loss: 9.664833 | Time: 10m 13s\n",
      "Epoch: 900 | Loss: 8.789612 | Time: 11m 30s\n",
      "Epoch: 1000 | Loss: 9.860293 | Time: 12m 49s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_epochs = 1000\n",
    "window_size = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start, time.time())\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is fruit really near to fish?\n",
    "Is fruit really far from cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rake',\n",
       " 'paths',\n",
       " 'awaken',\n",
       " 'fortunate',\n",
       " 'Religion',\n",
       " 'dance',\n",
       " 'delivering',\n",
       " 'territorial',\n",
       " 'grill',\n",
       " 'stripped',\n",
       " 'studying',\n",
       " 'folded',\n",
       " 'Walker',\n",
       " 'stack',\n",
       " 'two',\n",
       " 'Selden',\n",
       " 'involved',\n",
       " 'compatible',\n",
       " 'agrees',\n",
       " 'chilled',\n",
       " 'inventory',\n",
       " 'expand',\n",
       " 'islands',\n",
       " 'pulling',\n",
       " 'disciplined',\n",
       " 'major',\n",
       " 'subsystems',\n",
       " 'critic',\n",
       " 'bounce',\n",
       " 'precedent',\n",
       " 'coalition',\n",
       " 'dangers',\n",
       " 'athlete',\n",
       " 'resumed',\n",
       " 'Technology',\n",
       " 'teacher',\n",
       " 'Sharpe',\n",
       " 'choke',\n",
       " 'public',\n",
       " 'towel',\n",
       " 'Minister',\n",
       " 'plunged',\n",
       " 'gymnastics',\n",
       " 'obtaining',\n",
       " 'Gin',\n",
       " 'roof',\n",
       " 'success',\n",
       " 'pamphlets',\n",
       " \"Jess's\",\n",
       " 'assure',\n",
       " 'dream',\n",
       " 'Shall',\n",
       " 'magical',\n",
       " '$2',\n",
       " 'spends',\n",
       " 'studies',\n",
       " '1912',\n",
       " 'tight',\n",
       " 'shrugged',\n",
       " 'Diane',\n",
       " 'physical',\n",
       " 'conspiracy',\n",
       " 'electrons',\n",
       " 'fashion',\n",
       " 'essence',\n",
       " 'gyro',\n",
       " 'enters',\n",
       " 'aids',\n",
       " 'transom',\n",
       " 'leadership',\n",
       " 'Holden',\n",
       " 'clergymen',\n",
       " 'contest',\n",
       " 'liable',\n",
       " 'Eileen',\n",
       " 'lips',\n",
       " 'reared',\n",
       " 'shifted',\n",
       " 'March',\n",
       " 'pick',\n",
       " 'segregated',\n",
       " \"men's\",\n",
       " 'drug',\n",
       " 'Adams',\n",
       " 'Note',\n",
       " 'heap',\n",
       " 'quantitative',\n",
       " 'moods',\n",
       " 'authors',\n",
       " 'displays',\n",
       " 'Temple',\n",
       " 'Many',\n",
       " 'Kentucky',\n",
       " 'VA',\n",
       " 'appeal',\n",
       " 'Huff',\n",
       " 'NATO',\n",
       " \"kid's\",\n",
       " 'unsuccessful',\n",
       " 'gleaming',\n",
       " 'mortgage',\n",
       " 'refuse',\n",
       " 'Industries',\n",
       " 'vital',\n",
       " 'Data',\n",
       " 'walk',\n",
       " 'imposing',\n",
       " 'Monroe',\n",
       " 'happened',\n",
       " 'enabling',\n",
       " 'surface',\n",
       " 'Love',\n",
       " 'antigen',\n",
       " 'Benson',\n",
       " 'waiter',\n",
       " 'grass',\n",
       " 'stockholders',\n",
       " 'tips',\n",
       " 'audience',\n",
       " 'consensus',\n",
       " 'Pirates',\n",
       " 'apparent',\n",
       " 'submit',\n",
       " 'vertex',\n",
       " 'underground',\n",
       " 'beautiful',\n",
       " 'Poland',\n",
       " 'lawyer',\n",
       " 'nervous',\n",
       " 'route',\n",
       " 'handsome',\n",
       " 'shaft',\n",
       " 'plus',\n",
       " 'Presbyterian',\n",
       " 'mimesis',\n",
       " 'fourteen',\n",
       " 'factories',\n",
       " 'terribly',\n",
       " 'consider',\n",
       " 'finishing',\n",
       " 'Tammany',\n",
       " 'slip',\n",
       " 'spread',\n",
       " 'drained',\n",
       " 'ignorant',\n",
       " 'defend',\n",
       " 'personnel',\n",
       " 'fame',\n",
       " 'informal',\n",
       " 'realtors',\n",
       " 'absently',\n",
       " 'Dallas',\n",
       " 'abandon',\n",
       " 'require',\n",
       " 'deadly',\n",
       " 'homogeneous',\n",
       " 'eliminate',\n",
       " 'crisis',\n",
       " 'Miss',\n",
       " 'player',\n",
       " 'summarized',\n",
       " 'strides',\n",
       " 'Hohlbein',\n",
       " 'defining',\n",
       " 'slightest',\n",
       " 'formidable',\n",
       " 'Judy',\n",
       " 'counterparts',\n",
       " 'container',\n",
       " 'economy',\n",
       " 'traveling',\n",
       " 'meet',\n",
       " 'puzzle',\n",
       " 'host',\n",
       " \"I'm\",\n",
       " 'collections',\n",
       " 'Rockefeller',\n",
       " 'cash',\n",
       " 'communicative',\n",
       " 'Costaggini',\n",
       " 'empty',\n",
       " 'patrons',\n",
       " 'methods',\n",
       " 'Montpelier',\n",
       " 'loyalty',\n",
       " 'Fosdick',\n",
       " 'ready',\n",
       " 'observes',\n",
       " 'Information',\n",
       " 'share',\n",
       " 'command',\n",
       " 'One',\n",
       " 'guessing',\n",
       " 'comes',\n",
       " 'Morgan',\n",
       " 'Theorem',\n",
       " 'foams',\n",
       " 'distant',\n",
       " 'suddenly',\n",
       " 'Eleanor',\n",
       " 'southern',\n",
       " 'licensed',\n",
       " 'etc.',\n",
       " 'hollow',\n",
       " 'receiving',\n",
       " 'twentieth-century',\n",
       " 'assignments',\n",
       " 'transit',\n",
       " 'trailer',\n",
       " 'Way',\n",
       " 'Telegraph',\n",
       " 'medicine',\n",
       " 'perfume',\n",
       " 'narrowed',\n",
       " 'slept',\n",
       " 'realistically',\n",
       " 'Daily',\n",
       " 'color',\n",
       " 'Beowulf',\n",
       " 'peas',\n",
       " 'scenes',\n",
       " 'Ed',\n",
       " 'classroom',\n",
       " 'notches',\n",
       " 'Atlanta',\n",
       " 'bodies',\n",
       " 'provision',\n",
       " 'officially',\n",
       " 'considering',\n",
       " 'receipts',\n",
       " 'voluntarily',\n",
       " 'Living',\n",
       " 'simplicity',\n",
       " 'lawn',\n",
       " 'prevailing',\n",
       " 'satisfaction',\n",
       " 'adaptation',\n",
       " '**yf',\n",
       " 'son',\n",
       " 'schools',\n",
       " 'annoyed',\n",
       " 'rent',\n",
       " 'gentleman',\n",
       " 'colonial',\n",
       " 'cocktail',\n",
       " '1832',\n",
       " 'praise',\n",
       " 'slaves',\n",
       " 'Meredith',\n",
       " 'throw',\n",
       " 'Affairs',\n",
       " 'sovereignty',\n",
       " 'butter',\n",
       " 'brothers',\n",
       " 'communications',\n",
       " 'compensation',\n",
       " 'variety',\n",
       " 'properties',\n",
       " 'Science',\n",
       " 'drop',\n",
       " 'moon',\n",
       " 'traveled',\n",
       " 'scraped',\n",
       " 'filthy',\n",
       " 'Southerners',\n",
       " 'calculation',\n",
       " 'lands',\n",
       " 'nothing',\n",
       " 'loud',\n",
       " 'Children',\n",
       " 'condemned',\n",
       " 'method',\n",
       " 'response',\n",
       " 'sturdy',\n",
       " 'bacon',\n",
       " 'drill',\n",
       " 'ratings',\n",
       " 'sand',\n",
       " 'presenting',\n",
       " 'needed',\n",
       " 'allow',\n",
       " 'wrecked',\n",
       " 'Hanford',\n",
       " 'foe',\n",
       " 'powdered',\n",
       " 'congruence',\n",
       " 'centimeters',\n",
       " 'leader',\n",
       " 'miracle',\n",
       " 'Dickens',\n",
       " 'stores',\n",
       " 'fancy',\n",
       " 'TV',\n",
       " 'northern',\n",
       " 'deal',\n",
       " 'interrupted',\n",
       " 'fellows',\n",
       " 'forthcoming',\n",
       " 'prominently',\n",
       " 'previously',\n",
       " 'acting',\n",
       " 'teeth',\n",
       " 'Belgians',\n",
       " 'participation',\n",
       " 'exactly',\n",
       " 'Farm',\n",
       " 'opinion',\n",
       " 'proud',\n",
       " 'blocks',\n",
       " 'burns',\n",
       " 'directing',\n",
       " 'assign',\n",
       " 'thoughts',\n",
       " 'ruled',\n",
       " 'daytime',\n",
       " 'employed',\n",
       " 'Graham',\n",
       " 'deals',\n",
       " 'June',\n",
       " 'cigar',\n",
       " 'openings',\n",
       " 'woke',\n",
       " 'tie',\n",
       " 'carbon',\n",
       " 'wedding',\n",
       " 'joined',\n",
       " 'Rather',\n",
       " 'stretching',\n",
       " 'Brandon',\n",
       " 'arts',\n",
       " 'whisper',\n",
       " 'snow',\n",
       " 'character',\n",
       " 'planking',\n",
       " 'causes',\n",
       " 'ear',\n",
       " 'consulted',\n",
       " 'blond',\n",
       " 'Bishop',\n",
       " 'challenging',\n",
       " 'Otherwise',\n",
       " 'illustrations',\n",
       " '50',\n",
       " 'aerated',\n",
       " \"people's\",\n",
       " 'Still',\n",
       " 'nearby',\n",
       " 'populated',\n",
       " 'scrutiny',\n",
       " 'added',\n",
       " 'likewise',\n",
       " 'presses',\n",
       " 'regretted',\n",
       " 'Herbert',\n",
       " 'trailing',\n",
       " 'Pathet',\n",
       " 'Brooklyn',\n",
       " 'weaker',\n",
       " 'enormous',\n",
       " 'and/or',\n",
       " 'shopping',\n",
       " 'Nick',\n",
       " 'chairmen',\n",
       " 'seasons',\n",
       " \"club's\",\n",
       " 'Norman',\n",
       " 'movable',\n",
       " 'dimensional',\n",
       " 'potato',\n",
       " '10-year',\n",
       " 'Indies',\n",
       " 'sure',\n",
       " 'effort',\n",
       " 'Hot',\n",
       " 'Hoag',\n",
       " 'willingness',\n",
       " 'Wednesday',\n",
       " 'generations',\n",
       " 'restrict',\n",
       " 'Forest',\n",
       " 'pit',\n",
       " 'judicial',\n",
       " 'gonna',\n",
       " 'glory',\n",
       " 'substantially',\n",
       " 'terrible',\n",
       " 'automobiles',\n",
       " 'ballads',\n",
       " 'leveling',\n",
       " 'leaves',\n",
       " 'Yankees',\n",
       " 'severe',\n",
       " 'sprayed',\n",
       " 'witches',\n",
       " 'accompanied',\n",
       " 'Germany',\n",
       " 'applicants',\n",
       " 'cotton',\n",
       " 'employee',\n",
       " 'Fair',\n",
       " 'nobody',\n",
       " 'wanted',\n",
       " 'legs',\n",
       " 'ethnic',\n",
       " 'carpenter',\n",
       " 'taxi',\n",
       " 'beside',\n",
       " 'halfway',\n",
       " 'Union',\n",
       " 'educated',\n",
       " 'built',\n",
       " 'Try',\n",
       " 'nowhere',\n",
       " 'descending',\n",
       " 'summer',\n",
       " 'covers',\n",
       " 'investigators',\n",
       " 'Missouri',\n",
       " 'oldest',\n",
       " 'urgency',\n",
       " 'sliding',\n",
       " 'persuasion',\n",
       " 'mind',\n",
       " 'belly',\n",
       " 'accord',\n",
       " 'formulation',\n",
       " 'assessing',\n",
       " '6',\n",
       " 'wholesale',\n",
       " 'vitamins',\n",
       " 'bush',\n",
       " 'Saturday',\n",
       " 'enjoys',\n",
       " 'processing',\n",
       " 'Paula',\n",
       " 'pants',\n",
       " 'provide',\n",
       " \"women's\",\n",
       " 'trot',\n",
       " 'chewing',\n",
       " 'law',\n",
       " 'children',\n",
       " 'motivated',\n",
       " 'criterion',\n",
       " 'forced',\n",
       " 'flying',\n",
       " 'coffee',\n",
       " 'hill',\n",
       " 'melting',\n",
       " 'Y.',\n",
       " 'congregation',\n",
       " 'nickname',\n",
       " 'impartial',\n",
       " 'likely',\n",
       " 'urgent',\n",
       " 'exciting',\n",
       " 'restrictions',\n",
       " 'softly',\n",
       " 'bee',\n",
       " 'hearing',\n",
       " 'stop',\n",
       " 'Nobody',\n",
       " 'structure',\n",
       " 'either',\n",
       " 'Square',\n",
       " 'essay',\n",
       " 'individualism',\n",
       " 'guiding',\n",
       " 'grab',\n",
       " 'enacted',\n",
       " 'Either',\n",
       " 'paradoxically',\n",
       " 'noise',\n",
       " 'mouth',\n",
       " 'incorporated',\n",
       " 'earlier',\n",
       " 'spoiled',\n",
       " 'smoking',\n",
       " 'tallyho',\n",
       " 'occur',\n",
       " '$500',\n",
       " 'alibi',\n",
       " 'devoting',\n",
       " 'evaluated',\n",
       " 'Often',\n",
       " 'stared',\n",
       " 'sport',\n",
       " 'explode',\n",
       " 'dogs',\n",
       " 'two-thirds',\n",
       " 'curled',\n",
       " \"Thomas'\",\n",
       " 'stature',\n",
       " 'designing',\n",
       " 'fighters',\n",
       " 'resembled',\n",
       " 'fast',\n",
       " 'Cologne',\n",
       " 'N',\n",
       " 'Marty',\n",
       " 'attitudes',\n",
       " 'treatments',\n",
       " 'peeling',\n",
       " 'prediction',\n",
       " 'optimism',\n",
       " 'directed',\n",
       " 'carrying',\n",
       " 'consciousness',\n",
       " 'replace',\n",
       " 'Eisenhower',\n",
       " 'Apparently',\n",
       " 'alien',\n",
       " 'composition',\n",
       " 'hurry',\n",
       " 'installations',\n",
       " 'commercially',\n",
       " 'shortage',\n",
       " 'matched',\n",
       " 'gains',\n",
       " 'meats',\n",
       " 'unaware',\n",
       " 'distributed',\n",
       " '110',\n",
       " 'factors',\n",
       " 'Feathertop',\n",
       " 'particular',\n",
       " 'related',\n",
       " \"woman's\",\n",
       " 'Northeast',\n",
       " 'reported',\n",
       " 'sanctuary',\n",
       " 'historian',\n",
       " 'excluded',\n",
       " 'lid',\n",
       " 'clutching',\n",
       " 'reception',\n",
       " 'aid',\n",
       " 'Protestant',\n",
       " 'audiences',\n",
       " 'Lawrence',\n",
       " 'Lo',\n",
       " 'decides',\n",
       " 'tackle',\n",
       " 'hairs',\n",
       " 'morning',\n",
       " 'phone',\n",
       " 'haste',\n",
       " 'chart',\n",
       " 'present',\n",
       " 'assembly',\n",
       " 'boldly',\n",
       " 'Post',\n",
       " 'offset',\n",
       " 'study',\n",
       " 'neatly',\n",
       " 'drunk',\n",
       " 'woods',\n",
       " 'preventing',\n",
       " 'patience',\n",
       " 'forgiveness',\n",
       " 'Low',\n",
       " 'bitterness',\n",
       " 'enough',\n",
       " 'radius',\n",
       " 'deserved',\n",
       " 'advertised',\n",
       " 'silly',\n",
       " 'Association',\n",
       " 'Shakespeare',\n",
       " 'forest',\n",
       " 'symbolic',\n",
       " 'verse',\n",
       " 'spokesmen',\n",
       " 'carry',\n",
       " 'dominance',\n",
       " 'apparently',\n",
       " 'protein',\n",
       " 'promoting',\n",
       " 'perceptions',\n",
       " 'consultants',\n",
       " 'arises',\n",
       " 'files',\n",
       " 'daughters',\n",
       " 'profits',\n",
       " 'Schuylkill',\n",
       " 'economics',\n",
       " 'technicians',\n",
       " 'K.',\n",
       " 'director',\n",
       " 'convey',\n",
       " 'pony',\n",
       " 'lap',\n",
       " 'imagery',\n",
       " 'earthquakes',\n",
       " 'conveyed',\n",
       " 'Ekstrohm',\n",
       " 'exceeds',\n",
       " 'introduced',\n",
       " 'Thus',\n",
       " 'countenance',\n",
       " 'aside',\n",
       " 'likelihood',\n",
       " 'category',\n",
       " 'knit',\n",
       " '``',\n",
       " 'Luis',\n",
       " 'embarrassing',\n",
       " 'statistics',\n",
       " 'represents',\n",
       " 'blast',\n",
       " 'Butcher',\n",
       " 'socially',\n",
       " 'showing',\n",
       " 'disclosed',\n",
       " 'chapter',\n",
       " 'disappearance',\n",
       " \"we'd\",\n",
       " 'Sarah',\n",
       " 'asking',\n",
       " 'dust',\n",
       " 'believed',\n",
       " 'little',\n",
       " 'closely',\n",
       " 'Maria',\n",
       " 'Freud',\n",
       " 'technical',\n",
       " 'sons',\n",
       " 'adoption',\n",
       " 'Switzerland',\n",
       " 'strips',\n",
       " 'grinning',\n",
       " 'man',\n",
       " 'front',\n",
       " 'freed',\n",
       " 'evaluating',\n",
       " 'salesman',\n",
       " 'announcement',\n",
       " 'remained',\n",
       " 'administrative',\n",
       " 'grief',\n",
       " 'variables',\n",
       " 'Somers',\n",
       " 'Bears',\n",
       " 'businesses',\n",
       " 'patriotic',\n",
       " 'capable',\n",
       " 'intelligent',\n",
       " 'determines',\n",
       " 'obtain',\n",
       " 'partner',\n",
       " 'fragmentary',\n",
       " 'diversity',\n",
       " 'ideological',\n",
       " 'rows',\n",
       " 'task',\n",
       " 'desperate',\n",
       " 'faded',\n",
       " 'diameter',\n",
       " 'exports',\n",
       " 'shoes',\n",
       " 'wholesome',\n",
       " 'Pamela',\n",
       " 'slapped',\n",
       " 'consequence',\n",
       " 'twenty-one',\n",
       " 'satellite',\n",
       " 'New',\n",
       " 'thoroughly',\n",
       " 'Burke',\n",
       " 'seven',\n",
       " 'sell',\n",
       " 'wrist',\n",
       " 'crying',\n",
       " 'fighting',\n",
       " 'politely',\n",
       " 'modern',\n",
       " 'staged',\n",
       " 'taxes',\n",
       " 'Come',\n",
       " 'prohibition',\n",
       " 'comprehensive',\n",
       " 'Horn',\n",
       " 'characterization',\n",
       " 'programs',\n",
       " 'Burns',\n",
       " 'crutches',\n",
       " 'Moore',\n",
       " 'Jubal',\n",
       " 'sequence',\n",
       " 'argue',\n",
       " 'codes',\n",
       " 'maintain',\n",
       " 'compromise',\n",
       " 'guide',\n",
       " 'gratitude',\n",
       " 'baseball',\n",
       " 'pot',\n",
       " 'sheep',\n",
       " 'merits',\n",
       " 'films',\n",
       " 'box',\n",
       " 'passions',\n",
       " 'replies',\n",
       " 'peculiarly',\n",
       " 'waiting',\n",
       " 'attention',\n",
       " 'Review',\n",
       " 'Mike',\n",
       " 'panels',\n",
       " 'formulated',\n",
       " 'awoke',\n",
       " 'speech',\n",
       " 'distances',\n",
       " 'moderate',\n",
       " 'mess',\n",
       " 'rhythms',\n",
       " 'rally',\n",
       " 'football',\n",
       " \"She'll\",\n",
       " 'Wisman',\n",
       " 'muttered',\n",
       " 'missed',\n",
       " 'formations',\n",
       " 'brutality',\n",
       " 'air',\n",
       " 'locate',\n",
       " 'verdict',\n",
       " 'anti-slavery',\n",
       " 'improves',\n",
       " 'fine',\n",
       " 'recognize',\n",
       " 'envelope',\n",
       " \"It'll\",\n",
       " 'wrapped',\n",
       " 'checks',\n",
       " 'dead',\n",
       " 'jet',\n",
       " 'parameters',\n",
       " 'runs',\n",
       " 'notice',\n",
       " 'interest',\n",
       " 'constitution',\n",
       " 'immunity',\n",
       " 'smoke',\n",
       " 'governor',\n",
       " 'General',\n",
       " 'marched',\n",
       " 'catching',\n",
       " 'cholesterol',\n",
       " 'Mexican',\n",
       " 'contact',\n",
       " 'Interstate',\n",
       " 'enforce',\n",
       " 'long-range',\n",
       " 'importance',\n",
       " 'chocolate',\n",
       " 'minimum',\n",
       " 'Katie',\n",
       " 'Secretary',\n",
       " 'production',\n",
       " 'indignation',\n",
       " 'ballistic',\n",
       " 'ghetto',\n",
       " 'misery',\n",
       " 'Dead',\n",
       " 'release',\n",
       " 'wonderful',\n",
       " 'shaping',\n",
       " 'show',\n",
       " 'optimistic',\n",
       " 'recalled',\n",
       " 'lake',\n",
       " 'scrub',\n",
       " 'specifically',\n",
       " 'fault',\n",
       " 'Linda',\n",
       " 'foods',\n",
       " 'veil',\n",
       " '$100',\n",
       " 'Command',\n",
       " 'Army',\n",
       " 'dean',\n",
       " 'perceptual',\n",
       " 'moist',\n",
       " 'unpleasant',\n",
       " 'slavery',\n",
       " 'Gilborn',\n",
       " 'anybody',\n",
       " 'thrusting',\n",
       " 'ruin',\n",
       " 'clung',\n",
       " 'remove',\n",
       " 'producer',\n",
       " \"we're\",\n",
       " 'substantial',\n",
       " 'Georgia',\n",
       " 'observing',\n",
       " 'thrust',\n",
       " 'red',\n",
       " 'Sundays',\n",
       " 'Tory',\n",
       " 'assemble',\n",
       " 'Broadway',\n",
       " 'thanks',\n",
       " 'bunch',\n",
       " 'governmental',\n",
       " 'club',\n",
       " 'sensational',\n",
       " 'discipline',\n",
       " 'reflect',\n",
       " 'middle-aged',\n",
       " 'sentiment',\n",
       " 'using',\n",
       " 'makes',\n",
       " 'flexible',\n",
       " 'Rusk',\n",
       " 'dramatic',\n",
       " 'nuclei',\n",
       " 'Garth',\n",
       " 'Eventually',\n",
       " 'Sherman',\n",
       " 'judged',\n",
       " 'stimulus',\n",
       " 'pure',\n",
       " 'Suddenly',\n",
       " 'buried',\n",
       " 'questions',\n",
       " 'lives',\n",
       " 'Occasionally',\n",
       " 'leagues',\n",
       " 'criticized',\n",
       " 'danced',\n",
       " 'dancers',\n",
       " 'quarters',\n",
       " 'nitrogen',\n",
       " 'machines',\n",
       " 'rights',\n",
       " 'mahogany',\n",
       " 'compulsivity',\n",
       " 'loving',\n",
       " 'distal',\n",
       " 'dreamed',\n",
       " 'undesirable',\n",
       " 'solar',\n",
       " 'organic',\n",
       " 'Timothy',\n",
       " 'agency',\n",
       " 'carryover',\n",
       " 'thousands',\n",
       " 'dealer',\n",
       " 'protective',\n",
       " 'weekends',\n",
       " 'boom',\n",
       " 'expresses',\n",
       " 'marks',\n",
       " 'stood',\n",
       " 'similarly',\n",
       " 'joys',\n",
       " 'inquired',\n",
       " 'degree',\n",
       " 'Springs',\n",
       " 'six',\n",
       " 'Connecticut',\n",
       " 'evils',\n",
       " 'enjoyment',\n",
       " 'planned',\n",
       " 'Kremlin',\n",
       " 'twelve',\n",
       " 'Santa',\n",
       " 'saloons',\n",
       " 'Conference',\n",
       " 'stones',\n",
       " 'homer',\n",
       " 'Podger',\n",
       " 'qualities',\n",
       " 'sizes',\n",
       " 'radios',\n",
       " 'less',\n",
       " 'models',\n",
       " 'techniques',\n",
       " 'application',\n",
       " 'fuel',\n",
       " 'Council',\n",
       " 'interview',\n",
       " 'Woman',\n",
       " 'cooperate',\n",
       " 'knee',\n",
       " 'issues',\n",
       " 'Gen.',\n",
       " 'compared',\n",
       " 'Helen',\n",
       " 'painful',\n",
       " 'Anything',\n",
       " 'valuable',\n",
       " 'sunny',\n",
       " 'newspaper',\n",
       " 'fantastic',\n",
       " 'jaws',\n",
       " 'grasped',\n",
       " \"Fromm's\",\n",
       " 'participating',\n",
       " 'saw',\n",
       " 'Pulley',\n",
       " 'hawk',\n",
       " 'solve',\n",
       " 'voyage',\n",
       " 'tries',\n",
       " 'surprising',\n",
       " 'customs',\n",
       " 'ribbon',\n",
       " 'increasingly',\n",
       " 'pie',\n",
       " 'archaeology',\n",
       " 'storm',\n",
       " 'History',\n",
       " 'Students',\n",
       " 'denoted',\n",
       " 'precaution',\n",
       " 'arteries',\n",
       " 'miss',\n",
       " 'Davis',\n",
       " 'entrance',\n",
       " 'composer',\n",
       " 'Upton',\n",
       " 'turned',\n",
       " 'golden',\n",
       " 'misunderstanding',\n",
       " 'Dick',\n",
       " 'assigning',\n",
       " 'anaconda',\n",
       " 'adolescence',\n",
       " 'Would',\n",
       " 'fit',\n",
       " 'erect',\n",
       " 'Government',\n",
       " 'tasted',\n",
       " '75',\n",
       " 'cultivated',\n",
       " 'means',\n",
       " 'rounds',\n",
       " 'exploded',\n",
       " 'Fifth',\n",
       " 'sailing',\n",
       " 'capture',\n",
       " 'bargain',\n",
       " 'ponds',\n",
       " 'trunk',\n",
       " 'forgiven',\n",
       " 'concentration',\n",
       " 'coordination',\n",
       " 'Jay',\n",
       " 'consulting',\n",
       " 'ranges',\n",
       " 'guilty',\n",
       " 'string',\n",
       " 'courtesy',\n",
       " 'pleasant',\n",
       " 'kills',\n",
       " 'permanently',\n",
       " 'consisted',\n",
       " 'petitioner',\n",
       " 'cold',\n",
       " 'spot',\n",
       " 'nonspecific',\n",
       " 'gray',\n",
       " 'positive',\n",
       " 'comic',\n",
       " 'arrested',\n",
       " 'roles',\n",
       " 'corridor',\n",
       " 'inadequate',\n",
       " 'cancel',\n",
       " 'Patricia',\n",
       " 'pretending',\n",
       " 'newborn',\n",
       " 'unity',\n",
       " 'privileged',\n",
       " 'questionnaires',\n",
       " 'beef',\n",
       " 'sloping',\n",
       " 'Professor',\n",
       " 'object',\n",
       " 'space',\n",
       " 'anxiously',\n",
       " 'threw',\n",
       " 'noticed',\n",
       " 'breathe',\n",
       " 'attend',\n",
       " 'encounter',\n",
       " 'portion',\n",
       " 'liberties',\n",
       " 'queens',\n",
       " 'victory',\n",
       " 'brief',\n",
       " 'P',\n",
       " 'toll',\n",
       " 'completely',\n",
       " 'terms',\n",
       " 'Clayton',\n",
       " 'elsewhere',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4565])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish = torch.LongTensor([word2index['dog']])\n",
    "fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4720, -0.0374]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_embed_c = model.embedding_center(fish)\n",
    "fish_embed_o = model.embedding_outside(fish)\n",
    "fish_embed   = (fish_embed_c + fish_embed_o) / 2\n",
    "fish_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1423, 1.2924]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_embed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        \n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2218021154403687, -0.13546155393123627)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.060979366302490234, -0.26420873403549194)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4719747006893158, -0.037394583225250244)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.17060819268226624, 0.17068535089492798)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('fish')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.17060819268226624, 0.17068535089492798)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish = get_embed('fish')\n",
    "fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1793173998594284, 0.32553917169570923)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = get_embed('fruit')\n",
    "fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23981940746307373, -0.8369530439376831)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = get_embed('<UNK>')\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18377077966448851"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fish) @ np.array(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.874636699145703\n",
      "0.9605869379364954\n"
     ]
    }
   ],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "print(cosine_similarity(np.array(fish), np.array(unk)))\n",
    "print(cosine_similarity(np.array(fish), np.array(fruit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickle of the model\n",
    "import pickle\n",
    "\n",
    "with open('../../app/models/skipgram/skipgram.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('../../app/models/skipgram/skipgram_word2index.pkl', 'wb') as f:\n",
    "    pickle.dump(word2index, f)\n",
    "\n",
    "with open('../../app/models/skipgram/skipgram_index2word.pkl', 'wb') as f:\n",
    "    pickle.dump(index2word, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
