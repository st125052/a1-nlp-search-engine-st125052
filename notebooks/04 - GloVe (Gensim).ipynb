{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (Gensim)\n",
    "\n",
    "For looking at word vectors, we'll use **Gensim**. **Gensim** isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.   We gonna use **GloVe** embeddings, downloaded at [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Word2Vec(corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model in Word2Vec format for later use\n",
    "loaded_model.wv.save_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)\n",
    "\n",
    "# Load the model back for testing\n",
    "model = KeyedVectors.load_word2vec_format(\"brown_corpus_word2vec_format.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return the vectors\n",
    "model['coffee'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beer', 0.9718437790870667),\n",
       " ('mud', 0.9710042476654053),\n",
       " ('cloth', 0.9670085906982422),\n",
       " ('bid', 0.966858983039856),\n",
       " ('elephants', 0.9642542004585266),\n",
       " ('putt', 0.9636809825897217),\n",
       " ('bone', 0.961922824382782),\n",
       " ('target', 0.9612301588058472),\n",
       " ('cab', 0.9603677988052368),\n",
       " ('command', 0.9600244164466858)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comparison', 0.9648534655570984),\n",
       " ('congregation', 0.9621796011924744),\n",
       " ('character', 0.9620857834815979),\n",
       " ('danger', 0.9600976705551147),\n",
       " ('analysis', 0.9596229791641235),\n",
       " ('proof', 0.9583678841590881),\n",
       " ('tradition', 0.9559996128082275),\n",
       " ('ultimate', 0.9554035663604736),\n",
       " ('humanity', 0.9540858268737793),\n",
       " ('theory', 0.9535051584243774)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('energy', 0.9676889181137085),\n",
       " ('frame', 0.9576639533042908),\n",
       " ('annual', 0.9556906819343567),\n",
       " ('location', 0.9556302428245544),\n",
       " ('central', 0.954703152179718),\n",
       " ('forming', 0.9532434940338135),\n",
       " ('enterprise', 0.9522539377212524),\n",
       " ('diffusion', 0.9509708881378174),\n",
       " ('aid', 0.9498393535614014),\n",
       " ('farm', 0.9496110081672668)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiple meanings....\n",
    "model.most_similar(\"plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followers: 0.9489\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organized: 0.9656\n"
     ]
    }
   ],
   "source": [
    "#woman + king - man\n",
    "result = model.most_similar(positive=['woman', 'code'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "We have talked about this in the last class.  Here we can conveniently use `distance` to find the cosine distance between two words. Note that distance = 1 - similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1325734257698059, 0.08407062292098999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dog\"\n",
    "w2 = \"cat\"\n",
    "w3 = \"fruit\"\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#dog is much closer to cat then dog to fruit\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2810414433479309, 0.08063018321990967)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"happy\" # synonym 1\n",
    "w2 = \"cheerful\" # synonym 2\n",
    "w3 = \"sad\" # antonym\n",
    "w1_w2_dist = model.distance(w1, w2)\n",
    "w1_w3_dist = model.distance(w1, w3)\n",
    "\n",
    "#$w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\"!!\n",
    "#those similarlity does not handle antonym....\n",
    "w1_w2_dist, w1_w3_dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "You guys....one very important thing is that NLP models are biased.....very bad...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tract', 0.9730052351951599),\n",
      " ('Control', 0.966152548789978),\n",
      " ('African', 0.9653077125549316),\n",
      " ('Model', 0.9647234082221985),\n",
      " ('spectacular', 0.9641784429550171),\n",
      " ('megatons', 0.9639370441436768),\n",
      " ('biggest', 0.9634506702423096),\n",
      " ('splendid', 0.9634196162223816),\n",
      " ('assembly', 0.9632116556167603),\n",
      " ('WTV', 0.9631566405296326)]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(model.most_similar(positive=['woman', 'worker'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('originate', 0.8782269358634949),\n",
      " ('interfere', 0.8769674897193909),\n",
      " ('Everyone', 0.8759981393814087),\n",
      " ('promise', 0.8742619156837463),\n",
      " ('stress', 0.8734277486801147),\n",
      " ('stain', 0.8725233674049377),\n",
      " ('concentrate', 0.8714022040367126),\n",
      " ('Hiroshima', 0.8682627081871033),\n",
      " ('greatness', 0.8671994209289551),\n",
      " ('sign', 0.8631095290184021)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=['man', 'worker'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('device', 0.9400931000709534),\n",
      " ('Lord', 0.9389525055885315),\n",
      " ('abandoned', 0.9359262585639954),\n",
      " ('accepted', 0.9350138902664185),\n",
      " ('ended', 0.9346182346343994),\n",
      " ('conductor', 0.9342072010040283),\n",
      " ('magnificent', 0.9320111870765686),\n",
      " ('won', 0.9295842051506042),\n",
      " ('generation', 0.9290319085121155),\n",
      " ('released', 0.9280744791030884)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mankind', 0.9052137732505798),\n",
      " ('radiopasteurization', 0.8901543617248535),\n",
      " ('stain', 0.8867220282554626),\n",
      " ('justice', 0.8858324289321899),\n",
      " ('conformity', 0.8846319913864136),\n",
      " ('plumbing', 0.8841564059257507),\n",
      " ('promise', 0.8839837312698364),\n",
      " ('Madden', 0.8839766979217529),\n",
      " ('fellowship', 0.8827660083770752),\n",
      " ('innovation', 0.8827584981918335)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(model.most_similar(positive=[\"man\", \"code\"], negative=[\"woman\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'specifically'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'development'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('bird', 'fly', 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coke\n"
     ]
    }
   ],
   "source": [
    "#which word in the list does not belong\n",
    "print(model.doesnt_match(\"coke pepsi sprite water\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickle of the model\n",
    "import pickle\n",
    "\n",
    "with open('glove_gensim.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
